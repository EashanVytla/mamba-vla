#!/usr/bin/env python3
import logging
import os
import socket
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional, Sequence, Tuple, cast

import numpy as np
import torch

import hydra
from hydra.core.config_store import ConfigStore
from omegaconf import DictConfig, MISSING, OmegaConf

from vla_scratch.transforms.data_keys import PROCESSED_ACTION_KEY
from vla_scratch.datasets.config import DataConfig
from vla_scratch.policies.config import PolicyConfig, create_policy

from vla_scratch.transforms.data_types import Observation
from vla_scratch.transforms.base import TransformFn
from vla_scratch.helpers import (
    build_input_transforms,
    build_output_transforms,
    create_dataset,
)
from vla_scratch.utils.checkpoint import (
    find_latest_checkpoint,
    load_model_from_checkpoint,
)
from vla_scratch.datasets.libero.transforms import CatHistory
from vla_scratch.datasets.libero.common import STATE_KEY

from vla_scratch.serving.zmq_policy_server import ZmqPolicyServer
from vla_scratch.serving.websocket_policy_server import WebsocketPolicyServer
from vla_scratch.transforms.common import ToTorch, ToNumpy, ToObservation

logger = logging.getLogger(__name__)


@dataclass
class ServeConfig:
    defaults: list[Any] = field(
        default_factory=lambda: ["_self_", {"policy": "pi"}, {"data": "libero-ipec"}]
    )

    # server
    host: str = "0.0.0.0"
    port: int = 8000
    inference_steps: int = 10

    # configs
    data: DataConfig = MISSING
    policy: PolicyConfig = MISSING
    checkpoint_path: Optional[str] = None


cs = ConfigStore.instance()
cs.store(name="serve", node=ServeConfig())


class ServePolicy:
    def __init__(
        self,
        model: torch.nn.Module,
        input_transforms: Sequence[TransformFn],
        output_transforms: Sequence[TransformFn],
        inference_steps: int = 10,
    ) -> None:
        self._model = model
        self._num_steps = inference_steps
        self._device = next(model.parameters()).device
        self._input_transforms = input_transforms
        self._output_transforms = output_transforms

    @torch.inference_mode()
    def infer(self, obs: Dict[str, Any]) -> Dict[str, Any]:
        observation = obs
        for transform in self._input_transforms:
            observation = transform.compute(observation)
        assert isinstance(observation, Observation)
        observation = observation.to(self._device).unsqueeze(0)

        actions = self._model.sample_actions(observation, num_steps=self._num_steps)

        state_orig = torch.from_numpy(obs[STATE_KEY]).type(torch.float32)
        output = {
            PROCESSED_ACTION_KEY: actions.squeeze(0).cpu(),
            STATE_KEY: state_orig,
        }
        for transform in self._output_transforms:
            output = transform.compute(output)
        return output

    def reset(self) -> None:
        for input_transform in self._input_transforms:
            if isinstance(input_transform, CatHistory):
                input_transform.buffer.clear()


class ReplayPolicy:
    def __init__(
        self,
        dataset: torch.utils.data.Dataset,
        model: torch.nn.Module,
        input_transforms: Sequence[TransformFn],
        output_transforms: Sequence[TransformFn],
        inference_steps: int = 10,
    ) -> None:
        self._dataset = dataset
        self._counter = 0

        self._model = model
        self._num_steps = inference_steps
        self._device = next(model.parameters()).device
        self._num_steps = inference_steps
        self._input_transforms = input_transforms
        self._output_transforms = output_transforms

    @torch.inference_mode()
    def infer(self, obs: Dict[str, Any]) -> Dict[str, Any]:
        data_sample, _ = self._dataset[self._counter]
        actions = data_sample.action_chunk.actions.unsqueeze(0)
        self._counter += 1
        print(self._counter)

        state_orig = torch.from_numpy(obs[STATE_KEY]).type(torch.float32).unsqueeze(0)
        output = {
            PROCESSED_ACTION_KEY: actions.squeeze(0).cpu(),
            STATE_KEY: state_orig,
        }
        for transform in self._output_transforms:
            output = transform.compute(output)
        return output

    def reset(self) -> None:
        self._counter = 0


def _build_input_transforms(data_cfg: DataConfig, policy_cfg: PolicyConfig) -> Sequence[TransformFn]:
    # Preserve initial CatHistory + ToTorch behavior, then use shared helper
    base = [CatHistory(history=policy_cfg.state_history), ToTorch()]
    return list(base) + list(build_input_transforms(data_cfg, policy_cfg))


def _build_output_transforms(data_cfg: DataConfig, policy_cfg: PolicyConfig) -> Sequence[TransformFn]:
    return list(build_output_transforms(data_cfg, policy_cfg))


@hydra.main(config_name="serve", version_base=None)
def main(cfg: DictConfig) -> None:
    OmegaConf.resolve(cfg)
    OmegaConf.set_struct(cfg, False)

    # # Optionally override fields with saved configs from checkpoint dir
    # if cfg.get("checkpoint_path"):
    #     run_dir = Path(cast(str, cfg.checkpoint_path))
    #     saved_policy, saved_data = _load_saved_cfgs(run_dir)
    #     if saved_policy is not None:
    #         cfg.policy = OmegaConf.merge(cfg.policy, saved_policy)
    #     if saved_data is not None:
    #         cfg.data = OmegaConf.merge(cfg.data, saved_data)

    serve_cfg = cast(ServeConfig, OmegaConf.to_object(cfg))

    # Ensure data temporal parameters match policy
    serve_cfg.data.action_horizon = serve_cfg.policy.action_horizon
    serve_cfg.data.state_history = serve_cfg.policy.state_history

    dataset = create_dataset(
        serve_cfg.data,
        serve_cfg.policy,
    )

    dummy_data, _ = dataset[0]
    action_dim = dummy_data.action_chunk.actions.shape[-1]
    state_dim = dummy_data.observation.state.shape[-1]
    serve_cfg.policy.action_dim = action_dim
    serve_cfg.policy.state_dim = state_dim

    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info("Using device: %s", device)

    # Create model from policy config
    print("Initializing model...")
    with torch.device(device):
        model = create_policy(serve_cfg.policy)
    print("Model initialized.")

    # Load latest checkpoint
    if serve_cfg.checkpoint_path is not None:
        ckpt = find_latest_checkpoint(Path(serve_cfg.checkpoint_path))
        if ckpt is None:
            raise FileNotFoundError(
                f"No checkpoint found under {serve_cfg.checkpoint_path}"
            )
        print(f"Loading checkpoint: {ckpt}")
        missing, unexpected = load_model_from_checkpoint(model, ckpt, device, strict=False)
        print("Checkpoint loaded.")
        if missing:
            logger.warning("Missing keys when loading checkpoint: %s", missing)
        if unexpected:
            logger.warning("Unexpected keys when loading checkpoint: %s", unexpected)

    model.eval()

    # Build transforms
    input_transforms = _build_input_transforms(serve_cfg.data, serve_cfg.policy)
    output_transforms = _build_output_transforms(serve_cfg.data, serve_cfg.policy)

    # Wrap into serving policy
    policy = ServePolicy(
        model,
        input_transforms=input_transforms,
        output_transforms=output_transforms,
        inference_steps=serve_cfg.inference_steps,
    )
    # policy = ReplayPolicy(
    #     dataset,
    #     model,
    #     input_transforms=input_transforms,
    #     output_transforms=output_transforms,
    #     inference_steps=serve_cfg.inference_steps,
    # )

    metadata = {
        "policy": serve_cfg.policy._target_.split(".")[-1],
        "device": str(device),
    }

    # Warmup once to trigger initialization
    warmup = True
    if warmup:
        observation_in = {
            "observation/image": np.random.randint(
                0, 255, size=(3, 480, 640), dtype=np.uint8
            ),
            "observation/wrist_image": np.random.randint(
                0, 255, size=(3, 480, 640), dtype=np.uint8
            ),
            "observation/state": np.random.rand(serve_cfg.policy.state_dim).astype(
                np.float32
            ),
            "task": "Pick up the red block and place it on the green block.",
        }
        policy.infer(observation_in)
        policy.reset()

    server = ZmqPolicyServer(policy=policy, host=serve_cfg.host, port=serve_cfg.port, metadata=metadata)
    # server = WebsocketPolicyServer(policy=policy, host=serve_cfg.host, port=serve_cfg.port, metadata=metadata)

    hostname = socket.gethostname()
    local_ip = socket.gethostbyname(hostname)
    print(
        f"Serving policy {metadata.get('policy')} on {serve_cfg.host}:{serve_cfg.port} (host={hostname} ip={local_ip})",
    )

    server.serve_forever()


if __name__ == "__main__":
    main()
