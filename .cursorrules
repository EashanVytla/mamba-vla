# PyTorch & Performance Guidelines
- **Prioritize Vectorization:** Structure data processing to use `torch.cat`, `torch.stack`, or `view/reshape` over batch, time, and spatial dimensions. If a loop is necessary for logic or memory constraints, provide a brief justification or consider `torch.vmap`.
- **Minimize Host-Device Synchronization:**
    - Defer moving data to CPU (via `.item()`, `.tolist()`, or `.numpy()`) until logging or checkpointing steps.
    - Rely on static symbolic shapes where possible to maintain `torch.compile` compatibility. Avoid data-dependent shape checks inside critical paths.
    - Write code that is friendly to graph capture and kernel fusion.
- **Optimize Memory Usage:**
    - utilize in-place operations (`add_`, `mul_`) to reduce allocator overhead.
    - Default to `torch.inference_mode()` for validation and inference blocks.
- **Mamba/SSM Optimization:**
    - Construct inputs as contiguous memory blocks (e.g., `[Batch, Time, Features]`) to maximize scan kernel throughput.
    - Pass full tensors rather than lists of tensors to custom layers to reduce interpreter overhead.