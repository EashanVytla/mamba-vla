# PyTorch & Performance Guidelines
- **Vectorization is Mandatory:** Never use `for` loops over batch, time, or spatial dimensions. Always use `torch.cat`, `torch.stack`, or `view/reshape` to process data in bulk. If a loop seems unavoidable, justify it or ask if a vectorized alternative exists.
- **Avoid CUDA Syncs:**
    - Do NOT use `.item()`, `.tolist()`, or `.numpy()` inside the training loop unless absolutely necessary for logging.
    - Do NOT inspect tensor shapes via `tensor.shape` or `len(tensor)` inside critical loops if it forces a sync (e.g., if the shape depends on a data-dependent operation). Trust static shapes where possible.
    - Use `torch.compile` friendly patterns.
- **Memory Efficiency:**
    - Prefer in-place operations (`add_` instead of `add`) when possible.
    - Use `torch.inference_mode()` instead of `torch.no_grad()` for inference.
- **Mamba/SSM Specifics:**
    - When preparing inputs for SSMs, prefer preparing contiguous blocks of memory (e.g. `[Batch, Time, Features]`) rather than lists of tensors.