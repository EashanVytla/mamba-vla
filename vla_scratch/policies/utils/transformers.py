from typing import Tuple

import jaxtyping as at
import torch


def rotate_half(x: torch.Tensor) -> torch.Tensor:
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)


def apply_rotary_pos_emb(
    q: torch.Tensor,
    k: torch.Tensor,
    cos: torch.Tensor,
    sin: torch.Tensor,
    *,
    unsqueeze_dim: int = 1,
) -> Tuple[torch.Tensor, torch.Tensor]:
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def create_sinusoidal_pos_embedding(
    time: at.Float[torch.Tensor, " b"],  # noqa: F722
    dimension: int,
    min_period: float,
    max_period: float,
    device="cpu",
    dtype=torch.float32,
) -> at.Float[torch.Tensor, " b d"]:  # noqa: F722
    if dimension % 2 != 0:
        raise ValueError(f"dimension ({dimension}) must be divisible by 2")
    if time.ndim != 1:
        raise ValueError(
            "The time tensor is expected to be of shape `(batch_size,)`."
        )

    fraction = torch.linspace(
        0.0, 1.0, dimension // 2, dtype=dtype, device=device
    )
    period = min_period * (max_period / min_period) ** fraction
    scaling_factor = 1.0 / period * 2 * torch.pi
    sin_input = scaling_factor[None, :] * time[:, None]
    return torch.cat([torch.sin(sin_input), torch.cos(sin_input)], dim=1)


def make_att_2d_masks(
    pad_masks: at.Bool[torch.Tensor, " b n"],  # noqa: F722
    att_masks: at.Bool[torch.Tensor, " b n"],  # noqa: F722
) -> at.Bool[torch.Tensor, " b n n"]:  # noqa: F722
    if att_masks.ndim != 2:
        raise ValueError(att_masks.ndim)
    if pad_masks.ndim != 2:
        raise ValueError(pad_masks.ndim)

    cumsum = torch.cumsum(att_masks, dim=1)
    att_2d_masks = cumsum[:, None, :] <= cumsum[:, :, None]
    pad_2d_masks = pad_masks[:, None, :] * pad_masks[:, :, None]
    return att_2d_masks & pad_2d_masks
