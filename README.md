<div align="left">
  <h2>
  VLA-Scratch: 
  <!-- <br/> -->
  a Modular, Performant, Transparent Stack<br/>
  For Vision-Language-Action Models
  </h2>
</div>
<!-- add a huggingface badge, a twitter badge and a github star badge -->

## ‚ú® Key-Features

- **Explicit Data Model for Composable Modules** 
    - `TensorClass` is used to define explicit data boundaries between modules, making our codebase fully typed and modular. This allows heterogeneous dataset co-training, and [clearer data flow](vla_scratch/transforms/README.md) between datasets, policies, and transforms.
    ![data model](assets/data_model.png)
- **Dedicated Tuning for a First-Class Performance Stack** 
    - The [Qwen3-VL bridge](vla_scratch/policies/modules/vlm_bridge/qwen/bridge.py) rewrites the forward path to [minimize host-device syncs](vla_scratch/policies/README.md), making throughput 2x higher.
    - Layer-wise FSDP sharding and gradient checkpointing saves memory up to 2x, making it easier to scale model parameters.
    - TODO: figure here.
- **Clarity-Focused Hydra Workflow for Seamless Experimentation** 
    - Every [policy](vla_scratch/policies/pi/config.py) and [data](vla_scratch/datasets/config.py) config is registered with Hydra's `ConfigStore`, so experiments are overrideable with minimal boilerplate.
    - Training, eval, and serving scripts share a common config grammar, so switching between workflows is seamless.
    - TODO: example script snippet here.


## üóÇÔ∏è Codebase Structure

VLA-Scratch is a fully modular, high-performance VLA stack built around Hydra configs, TensorClass data models, and reusable training helpers.

| Path | Description | Docs |
| --- | --- | --- |
| `vla_scratch/transforms/` | Data transforms and TensorClass models | [Documentation ‚Üí](vla_scratch/transforms/README.md) |
| `vla_scratch/datasets/` | Dataset loaders and transforms | [Documentation ‚Üí](vla_scratch/datasets/README.md) |
| `vla_scratch/policies/` | Policy interfaces, bridges, and action heads | [Documentation ‚Üí](vla_scratch/policies/README.md) |
| `scripts/` | Training/eval/serving scripts with shared Hydra grammar | [Documentation ‚Üí](scripts/README.md) |
<!-- - `vla_scratch/helpers/` ‚Äî repo-aware data + training helpers (prefetch dataloaders, FSDP setup, normalization I/O).
- `vla_scratch/utils/` ‚Äî standalone utilities (config discovery, checkpointing, filesystem paths) that avoid repo-level imports.
- `examples/` ‚Äî ready-to-run experiment folders demonstrating LIBERO usage and viz workflows.
- `tests/` ‚Äî smoke tests and unit tests for policies, transforms, and helpers. -->

## üöÄ Quickstart

```bash
# Setup virtual environment and install dependencies:
uv sync
source .venv/bin/activate

# Apply lerobot dataset patch https://github.com/huggingface/lerobot/issues/959
patch -N -d .venv/lib/python3.10/site-packages/lerobot/datasets -p0 < ./patches/lerobot_dataset.patch
```

### Training
```bash
torchrun --standalone --nnodes=1 --nproc_per_node=4 \
    train_policy.py \
    policy=pi-qwen \
    policy.state_history=0 \
    policy.action_horizon=10 \
    policy.use_state=False \
    policy.transforms.0.max_length=500 \
    data=dont_blind \
    eval_data=dont_blind_8_8_eval \
    batch_size=32 \
    lr.base=3e-6 \
    +lr.vlm_bridge=3e-7 \
    +lr.action_expert=3e-6 \
    wandb.mode=online
```

<!-- 

```bash
hf download IPEC-COMMUNITY/libero_spatial_no_noops_1.0.0_lerobot --repo-type dataset

export HF_CACHE_TEMP=/home/elijah/.cache/huggingface/
ln -s ${HF_CACHE_TEMP}/hub/datasets--IPEC-COMMUNITY--libero_spatial_no_noops_1.0.0_lerobot/snapshots/cb7508999d4a8caa65b6448d6d700e1f347b809e ${HF_CACHE_TEMP}/lerobot/IPEC-COMMUNITY/libero_spatial_no_noops_1.0.0_lerobot
```
 -->
